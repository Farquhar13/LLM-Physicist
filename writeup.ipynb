{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b9e61b",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "The broader motivation is to create an AI physicist: a system that helps physicists in their research, or even discovers new physics in a self-guided way.\n",
    "\n",
    "Given the current state of AI, a resonable system design entails a central LLM that has a strong understanding of physics with acccess to external tools such as RAG over literature, and/or access to additional tools such as math libraries, heuristic solvers, the ability to write and run code, or proof-checking software.\n",
    "\n",
    "Motivated by this direction, we take the central LLM as a starting point: **_We take a 1B parameter model, assess it's output on QA-style (question/answer) conceptual physics problems, and show a pipeline to do fine-tuning with LoRA with evaluation metrics._**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552affca",
   "metadata": {},
   "source": [
    "# 1. Load a 1B parameter model. Test LoRA pipeline. Show example output.\n",
    "\n",
    "For our starting point we'll use the Llama-3.2-1B-Instruct model. Models with 1 billion parameters represent an interesting space. They don't have the capabilities and understanding of larger models, but they are large enough to give answers that are on the right the track while still being small enough to run locally which is good for fast prototyping. Additionally, the intstruction-training should be beneficial for our purposes as it should improves the models reasoning abilities and performance on QA tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecd36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Fine-tune LLama-3.2-1B-Instruct with LoRA on a small set of questions\n",
    "\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import Dataset\n",
    "\n",
    "# --- Load Llama 3.2 1B Instruct model (full precision) ---\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cpu\"\n",
    ")\n",
    "\n",
    "# --- Add LoRA adapter (lightweight fine-tune) ---\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# --- Tiny physics dataset ---\n",
    "train_data = [\n",
    "    {\"instruction\": \"Explain black hole entropy\",\n",
    "     \"output\": \"Black hole entropy is proportional to the event horizon area via S = kA/(4ℓ_p²).\"},\n",
    "    {\"instruction\": \"State Schrödinger’s equation\",\n",
    "     \"output\": \"iħ ∂ψ/∂t = Ĥψ describes quantum system evolution.\"},\n",
    "    {\"instruction\": \"What is Noether’s theorem?\",\n",
    "     \"output\": \"Noether's theorem links continuous symmetries to conserved quantities like momentum or energy.\"}\n",
    "]\n",
    "\n",
    "def format_examples(batch):\n",
    "    return {\n",
    "        \"text\": [\n",
    "            f\"Question: {instr}\\nAnswer: {out}\"\n",
    "            for instr, out in zip(batch[\"instruction\"], batch[\"output\"])\n",
    "        ]\n",
    "    }\n",
    "\n",
    "dataset = Dataset.from_list(train_data).map(format_examples, batched=True)\n",
    "\n",
    "def tokenize(batch):\n",
    "    encodings = tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "    encodings[\"labels\"] = encodings[\"input_ids\"].copy()\n",
    "    return encodings\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=True)\n",
    "\n",
    "# --- Training arguments ---\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=5,\n",
    "    max_steps=50, \n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    output_dir=\"./llama1b-phys-lora\",\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset\n",
    ")\n",
    "\n",
    "# --- Train ---\n",
    "trainer.train()\n",
    "\n",
    "# --- Test ---\n",
    "prompt = \"Explain black hole entropy in simple terms.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=500)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f628c45",
   "metadata": {},
   "source": [
    "## Example Ouputs: prompt = \"Explain black hole entropy in simple terms.\"\n",
    "\n",
    "#### Example 1 (max_new_tokens=100)\n",
    "\n",
    "Step 1: Define entropy\n",
    "Entropy is a measure of disorder or randomness in a system. In thermodynamics, entropy is used to quantify the amount of thermal energy unavailable to do work in a system.\n",
    "\n",
    "Step 2: Introduce black hole entropy\n",
    "Black hole entropy is a concept introduced by Stephen Hawking in the 1970s. It is a measure of the disorder or randomness in a black hole, which is a region of spacetime with such intense gravitational pull that not\n",
    "\n",
    "#### Example 2 (max_new_tokens=500)\n",
    "\n",
    "Step 1: Define entropy\n",
    "Entropy is a measure of disorder or randomness in a system.\n",
    "\n",
    "Step 2: Explain black hole entropy\n",
    "In a black hole, the entropy is proportional to the surface area of the event horizon, which is the boundary of the black hole.\n",
    "\n",
    "Step 3: Provide a simple analogy\n",
    "Think of a black hole as a vacuum cleaner that sucks up everything around it, including information and energy. The more matter and energy that's sucked into the black hole, the more entropy it has.\n",
    "\n",
    "Step 4: Quantify the entropy\n",
    "The entropy of a black hole is proportional to the surface area of its event horizon, which is given by the formula: S = A / (4G), where S is the entropy, A is the surface area of the event horizon, and G is the gravitational constant.\n",
    "\n",
    "Step 5: Summarize the concept\n",
    "Black hole entropy is proportional to the surface area of the event horizon, and it increases as matter and energy are added to the black hole.\n",
    "\n",
    "The final answer is: $\\boxed{S = A / (4G)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66989600",
   "metadata": {},
   "source": [
    "##  Output Analysis\n",
    "* Structure: In the answers we can see the step-by-step format of the answers. This structured response is a result of the instruction-tuning performed on the base model.\n",
    "* Tuning: In the first example, the output was incomplete, but is easily fixable by increasing the maximum number of tokens allowed for the model's output.\n",
    "* Quality: As a high-level conceptual answer, it's not bad. It's clear, simple, and generally accurate. However there are aspects that could be improved. While it's a good pedagogy to break down the concept into simplier chunks, the formatting could be cleaner, for example, the step-structure my not contribute much here. The model says what's it's going to explain before it does it, which isn't necessary and could be improved. Also the formula it provides is simplified with no mention of the missing factors. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b54681",
   "metadata": {},
   "source": [
    "# 2. LoRA tuning on a larger dataset. Introduce evaluation metrics.\n",
    "\n",
    "In the previous example we used a tiny custom dataset to demonstrate the LoRA fine-tuning process. Now we'll train on a larger dataset (although still small by LLM standards), introduce some perfromance metrics to try to quanity the effects of fine-tuning by evaluating before and after training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f882d4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-tune Llama-3.2-1B-Instruct on physics Q&A (veggiebird/physics-scienceqa)\n",
    "Demonstrates pre- and post-fine-tuning outputs using LoRA (CPU-friendly).\n",
    "Includes quantitative evaluation (Exact Match + ROUGE-L) and sample outputs.\n",
    "\"\"\"\n",
    "\n",
    "import evaluate\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Load base model + tokenizer\n",
    "# -------------------------------\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"cpu\"  # CPU-only; fine for 1B LoRA demo\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Load and format dataset\n",
    "# -------------------------------\n",
    "\n",
    "raw_ds = load_dataset(\"veggiebird/physics-scienceqa\", split=\"train\")\n",
    "\n",
    "# Convert to Q/A format\n",
    "def format_batch(batch):\n",
    "    return {\n",
    "        \"text\": [\n",
    "            f\"Question: {q}\\nAnswer: {a}\"\n",
    "            for q, a in zip(batch[\"input\"], batch[\"output\"])\n",
    "        ]\n",
    "    }\n",
    "\n",
    "formatted_ds = raw_ds.map(format_batch, batched=True)\n",
    "\n",
    "# Tokenize with labels\n",
    "def tokenize(batch):\n",
    "    enc = tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=256)\n",
    "    enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
    "    return enc\n",
    "\n",
    "tokenized_ds = formatted_ds.map(tokenize, batched=True)\n",
    "\n",
    "# Split train/test\n",
    "split = tokenized_ds.train_test_split(test_size=0.1, seed=42)\n",
    "train_ds = split[\"train\"]\n",
    "test_ds = split[\"test\"]\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Evaluation utilities\n",
    "# -------------------------------\n",
    "\n",
    "# Exact Match\n",
    "def exact_match(pred, ref):\n",
    "    return 1 if pred.strip().lower() == ref.strip().lower() else 0\n",
    "\n",
    "# ROUGE-L\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def evaluate_model(model, tokenizer, dataset, n_samples=50): # increase n_samples later\n",
    "    em_scores = []\n",
    "    rouge_scores = []\n",
    "    subset = dataset.shuffle(seed=42).select(range(min(n_samples, len(dataset))))\n",
    "\n",
    "    for ex in subset:\n",
    "        # Extract Q/A from formatted text\n",
    "        question = ex[\"text\"].split(\"\\nAnswer:\")[0].replace(\"Question: \", \"\")\n",
    "        true_answer = ex[\"text\"].split(\"\\nAnswer:\")[1]\n",
    "\n",
    "        # Generate prediction\n",
    "        inputs = tokenizer(question, return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(**inputs, max_new_tokens=100, pad_token_id=tokenizer.eos_token_id)\n",
    "        pred_answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        # Metrics\n",
    "        em_scores.append(exact_match(pred_answer, true_answer))\n",
    "        rouge_result = rouge_metric.compute(predictions=[pred_answer], references=[true_answer])\n",
    "        rouge_scores.append(rouge_result[\"rougeL\"])\n",
    "\n",
    "    return {\n",
    "        \"Exact Match\": np.mean(em_scores),\n",
    "        \"ROUGE-L\": np.mean(rouge_scores)\n",
    "    }\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Pre-fine-tune evaluation\n",
    "# -------------------------------\n",
    "\n",
    "print(\"\\n=== Evaluating base model ===\")\n",
    "base_metrics = evaluate_model(model, tokenizer, test_ds)\n",
    "print(base_metrics)\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Apply LoRA adapter\n",
    "# -------------------------------\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Fine-tuning\n",
    "# -------------------------------\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama1b-phys-scienceqa\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    warmup_steps=10,\n",
    "    max_steps=200,       # originall 200, moving down for testing\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds\n",
    ")\n",
    "\n",
    "print(\"\\nStarting fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# -------------------------------\n",
    "# 7. Post-fine-tune evaluation\n",
    "# -------------------------------\n",
    "\n",
    "print(\"\\n=== Evaluating fine-tuned model ===\")\n",
    "ft_metrics = evaluate_model(model, tokenizer, test_ds)\n",
    "print(ft_metrics)\n",
    "\n",
    "# -------------------------------\n",
    "# 8. Qualitative sample outputs\n",
    "# -------------------------------\n",
    "\n",
    "sample_questions = [\n",
    "    \"What is the second law of thermodynamics?\",\n",
    "    \"Explain Newton's third law of motion.\",\n",
    "    \"What happens to time near the speed of light?\"\n",
    "]\n",
    "\n",
    "print(\"\\n=== Fine-tuned model sample outputs ===\")\n",
    "for q in sample_questions:\n",
    "    inputs = tokenizer(q, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=200, pad_token_id=tokenizer.eos_token_id)\n",
    "    print(f\"\\nQ: {q}\\nA: {tokenizer.decode(outputs[0], skip_special_tokens=True)}\")\n",
    "\n",
    "# -------------------------------\n",
    "# 9. (Optional) Save adapter\n",
    "# -------------------------------\n",
    "# model.save_pretrained(\"./llama1b-phys-scienceqa-adapter\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1d6e5c",
   "metadata": {},
   "source": [
    "## Training Info:\n",
    "\n",
    "| Step | Training Loss |\n",
    "|------|---------------|\n",
    "| 20   | 3.122800      |\n",
    "| 40   | 0.771700      |\n",
    "| 60   | 0.497300      |\n",
    "| 80   | 0.455800      |\n",
    "| 100  | 0.345200      |\n",
    "| 120  | 0.346500      |\n",
    "| 140  | 0.311000      |\n",
    "| 160  | 0.312300      |\n",
    "| 180  | 0.251200      |\n",
    "| 200  | 0.235400      |\n",
    "\n",
    "We see a drop in training loss, indicating positive learning performance in fine-tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4f8b9",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "| Model              | ROUGE-L |\n",
    "|--------------------|---------|\n",
    "| Base (1B)          | 0.1950  |\n",
    "| Fine-tuned (1B)    | 0.2488  |\n",
    "\n",
    "The ROUGE-L metric goes from [0, 1] with higher scores representing better similarity to the reference dataeset. Here we see that fine-tuning improved the ROUGE-L score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a497272a",
   "metadata": {},
   "source": [
    "## Example Outputs (fine-tuned model):\n",
    "\n",
    "Q: What is the second law of thermodynamics?\n",
    "\n",
    "A: What is the second law of thermodynamics? The second law of thermodynamics is a statement about the direction of spontaneous processes. The law states that it is impossible to build a machine that can convert all the heat energy put into it into useful work. In other words, the heat energy from a machine will always be lost. This law applies to any spontaneous process.\n",
    "\n",
    "===\n",
    "\n",
    "Q: Explain Newton's third law of motion.\n",
    "\n",
    "A: Explain Newton's third law of motion. Describe how the law applies to everyday life.\n",
    "Newton's third law states that for every force, there is an equal and opposite force. This means that when you push on someone or something, the person or thing pushes back on you with the same force. Imagine pushing a friend's hand. The hand pushes back on your hand with the same force. The hand is pushing on you with a force that is equal in magnitude and direction to the force you applied to it.\n",
    "The third law applies to everyday life in many ways. For example, if you push on a wall, the wall pushes on you with a force of equal magnitude. The wall is pushing on you with a force that is equal in magnitude and direction to the force you applied to it. This means that the wall is not going to fall over or push you away. The wall is just going to push back on you with the same force.\n",
    "The third law also applies to the forces that act on objects in the Earth's atmosphere.\n",
    "\n",
    "===\n",
    "\n",
    "Q: What happens to time near the speed of light?\n",
    "\n",
    "A: What happens to time near the speed of light? The speed of light is a fundamental constant in physics. It is the fastest speed at which any object or information can travel. If you are moving at a speed greater than the speed of light, you will experience time dilation. Time dilation occurs when you are moving at a speed greater than the speed of light relative to an observer. The observer will measure time passing slower for you. This effect is very small at low speeds, but it becomes significant at high speeds.\n",
    "According to the theory of relativity, time dilation is a consequence of the speed of light. The closer you are to the speed of light, the slower time will pass for you relative to other observers. This means that time will pass slower for you relative to other observers. Time dilation is a predicted effect by Albert Einstein and is a key feature of his theory of special relativity.\n",
    "If you are moving at a speed greater than the speed of light, you will experience time dilation. The closer you are to the speed of light,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d07046",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "* Format: Overall good. Could use some improvements. For example, the prompt is unnecessarily repeated in the answer. Interestingly, we see that the base-model no longer uses the step-by-step format it used in the first experiement. This shows the effect of fine-tuning as the answers now more closely match the structure of the dataset used for fine-tuning.\n",
    "* Still getting some issues with answers being cut off before compeleting in the 3rd question. As before, we can increase the `max_new_tokens` to fix this.\n",
    "* Quality: The quality makes sense for a smaller LLM. It produces answers that at first-glance look pretty accruate, and much of the information it provides is accurate and clear. However, it clearly makes some conceptual errors, for example in the 3rd question it speaks of going faster than the speed of light."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c241c",
   "metadata": {},
   "source": [
    "# 3. Conclusions / Future Directions\n",
    "\n",
    "We demonstrated a process for fine-tuning a base LLM with LoRA on physics-specific data. Overall the models do pretty well as giving reasonable-sounding answers to conceptual physics questions, but at this scale are not capable of showing true expert-level knowledge and reliablility. \n",
    "\n",
    "We used a model with 1 billion parameters for ease-of-demonstration, but to improve performance the next steps are to increase model size and use more larger physics-specific datatsets for fine-tuning. Simply using a larger model will greatly increase performance in terms of the general knowledge required to answer these questions.\n",
    "\n",
    "Here we focused on the base LLM, but the broader goal would be to integrate an LLM with a broader set of tools. Nonetheless, fine-tuning the base model so that it's understanding of physics is as good as possible is critial step towards this bigger vision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b171269",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-physicist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
