{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b26d618",
   "metadata": {},
   "source": [
    "## 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cbfcb6",
   "metadata": {},
   "source": [
    "### Datasets\n",
    "* PhysicsQA : https://www.kaggle.com/datasets/mohammadbinaftab/physicsqa\n",
    "* MMLU - Massive Multitask Language Understanding : https://huggingface.co/datasets/cais/mmlu (has some physics categories)\n",
    "* Custom symbolic derivation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30583d76",
   "metadata": {},
   "source": [
    "### RAG\n",
    "* papers\n",
    "* textbooks\n",
    "* Physics StackExchange\n",
    "\n",
    "Consider OpenWebUI platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2687c96c",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f398780",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Lots of potential choices here. Probably better in general to use the largest and most capable base model that is feasible in terms of computational resources.\n",
    "\n",
    "First ideas: \n",
    "* Llama-3-8B\n",
    "* Llama-3.2-3B-Instruct\n",
    "* Llama‑3.2‑1B‑Instruct \n",
    "* Mistral-7B\n",
    "* GPT-J\n",
    "\n",
    "Instruction-tuned model might be better to fine-tune for physics. It should be better at Q/A and reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d073734c",
   "metadata": {},
   "source": [
    "## 3. Agentic tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14d34fd",
   "metadata": {},
   "source": [
    "* SymPy coding agent\n",
    "* Math proof checker\n",
    "* Domain specific packages (E.g., QuTip for quantum information, or Psi4 for quantum chemistry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a02c2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
